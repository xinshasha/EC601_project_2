{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.models import Model,load_model\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.layers import Input,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Activation,Dense,Flatten,Add\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data=pd.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.array(training_data['data'][0:int(training_data.shape[0]*4/5)].tolist())\n",
    "train_label=np.array(training_data['labels'][0:int(training_data.shape[0]*4/5)].tolist())\n",
    "val_data=np.array(training_data['data'][int(training_data.shape[0]*4/5):(training_data.shape[0])].tolist())\n",
    "val_label=np.array(training_data['labels'][int(training_data.shape[0]*4/5):(training_data.shape[0])].tolist())\n",
    "#split data in to training set and validation set as 4:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffin=[]\n",
    "fin=[]\n",
    "tmp=[];\n",
    "ttmp=[];\n",
    "for i in train_data:\n",
    "    for k in range(32):\n",
    "        for j in range(32):\n",
    "            ttmp.append(i[k*32+j]/255.0)\n",
    "            ttmp.append(i[k*32+j+1024]/255.0)\n",
    "            ttmp.append(i[k*32+j+2048]/255.0)\n",
    "            tmp.append(ttmp)\n",
    "            ttmp=[]\n",
    "        fin.append(tmp)\n",
    "        tmp=[]\n",
    "    ffin.append(fin)\n",
    "    fin=[]\n",
    "\n",
    "train_data=np.array(ffin)\n",
    "\n",
    "#convert train_data from 1x3072 into 32x32x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffin=[]\n",
    "fin=[]\n",
    "tmp=[];\n",
    "ttmp=[];\n",
    "for i in val_data:\n",
    "    for k in range(32):\n",
    "        for j in range(32):\n",
    "            ttmp.append(i[k*32+j]/255.0)\n",
    "            ttmp.append(i[k*32+j+1024]/255.0)\n",
    "            ttmp.append(i[k*32+j+2048]/255.0)\n",
    "            tmp.append(ttmp)\n",
    "            ttmp=[]\n",
    "        fin.append(tmp)\n",
    "        tmp=[]\n",
    "    ffin.append(fin)\n",
    "    fin=[]\n",
    "    \n",
    "val_data=np.array(ffin)\n",
    "ffin=[]\n",
    "\n",
    "#convert val_data from 1x3072 into 32x32x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label=np_utils.to_categorical(train_label, 2)\n",
    "val_label=np_utils.to_categorical(val_label, 2)\n",
    "#convert label into hot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=Input((32,32,3))\n",
    "'''\n",
    "c0 = Conv2D(16, (3, 3), activation='relu', padding='same') (input_layer)\n",
    "c0 = Conv2D(16, (3, 3), activation='relu', padding='same') (c0)\n",
    "p0 = Dropout(0.3) (c0)\n",
    "\n",
    "c1 = Conv2D(16, (3, 3), activation='relu', padding='same') (p0)\n",
    "c1 = Conv2D(16, (3, 3), activation='relu', padding='same') (c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = Dropout(0.3) (c1)\n",
    "p1=MaxPooling2D()(p1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = Conv2D(32, (3, 3), activation=None, padding='same') (c2)\n",
    "rc2 = BatchNormalization()(c2)\n",
    "rc2 = Activation('relu')(rc2)\n",
    "rc2 = Conv2D(32, (3, 3), activation='relu', padding='same') (rc2)\n",
    "rc2 = Conv2D(32, (3, 3), activation='relu', padding='same') (rc2)\n",
    "rc2 = Add() ([c2,rc2])\n",
    "p2 = Dropout(0.5) (rc2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(64, (3, 3), activation=None, padding='same') (c3)\n",
    "rc3 = BatchNormalization()(c3)\n",
    "rc3 = Activation('relu')(rc3)\n",
    "rc3 = Conv2D(64, (3, 3), activation='relu', padding='same') (rc3)\n",
    "rc3 = Conv2D(64, (3, 3), activation='relu', padding='same') (rc3)\n",
    "p2 = Dropout(0.5) (rc3)\n",
    "\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(64, (3, 3), activation=None, padding='same') (c3)\n",
    "rc3 = BatchNormalization()(c3)\n",
    "rc3 = Activation('relu')(rc3)\n",
    "rc3 = Conv2D(64, (3, 3), activation='relu', padding='same') (rc3)\n",
    "rc3 = Conv2D(64, (3, 3), activation='relu', padding='same') (rc3)\n",
    "\n",
    "p2 = Dropout(0.5) (rc3)\n",
    "\n",
    "\n",
    "c3 = Conv2D(128, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(128, (3, 3), activation=None, padding='same') (c3)\n",
    "rc3 = BatchNormalization()(c3)\n",
    "rc3 = Activation('relu')(rc3)\n",
    "rc3 = Conv2D(128, (3, 3), activation='relu', padding='same') (rc3)\n",
    "rc3 = Conv2D(128, (3, 3), activation='relu', padding='same') (rc3)\n",
    "\n",
    "'''\n",
    "nerual=16       #64 is worse than above network\n",
    "\n",
    "fconv=Conv2D(64, (7, 7), strides=(2,2),activation='relu', padding='same') (input_layer)\n",
    "\n",
    "c0 = Conv2D(nerual, (3, 3), activation='relu', padding='same') (fconv)\n",
    "c0 = Conv2D(nerual, (3, 3), activation='relu', padding='same') (c0)\n",
    "p0 = Dropout(0.1) (c0)\n",
    "\n",
    "c1 = Conv2D(nerual, (3, 3), activation='relu', padding='same') (p0)\n",
    "c1 = Conv2D(nerual, (3, 3), activation='relu', padding='same') (c1)\n",
    "c1 = Conv2D(nerual, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = Dropout(0.1) (c1)\n",
    "p1=MaxPooling2D()(p1)\n",
    "\n",
    "c2 = Conv2D(nerual*2, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = Conv2D(nerual*2, (3, 3), activation=None, padding='same') (c2)\n",
    "rc2 = BatchNormalization()(c2)\n",
    "rc2 = Activation('relu')(rc2)\n",
    "rc2 = Conv2D(nerual*2, (3, 3), activation='relu', padding='same') (rc2)\n",
    "rc2 = Conv2D(nerual*2, (3, 3), activation='relu', padding='same') (rc2)\n",
    "#rc2 = Add() ([c2,rc2])\n",
    "p2 = Dropout(0.5) (rc2)\n",
    "\n",
    "c3 = Conv2D(nerual*2, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(nerual*2, (3, 3), activation=None, padding='same') (c3)\n",
    "rc3 = BatchNormalization()(c3)\n",
    "rc3 = Activation('relu')(rc3)\n",
    "rc3 = Conv2D(nerual*2, (3, 3), activation='relu', padding='same') (rc3)\n",
    "rc3 = Conv2D(nerual*2, (3, 3), activation='relu', padding='same') (rc3)\n",
    "#rc3 = Add() ([c3,rc3])\n",
    "p2 = Dropout(0.5) (rc3)\n",
    "\n",
    "c3 = Conv2D(nerual*4, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(nerual*4, (3, 3), activation=None, padding='same') (c3)\n",
    "rc3 = BatchNormalization()(c3)\n",
    "rc3 = Activation('relu')(rc3)\n",
    "rc3 = Conv2D(nerual*4, (3, 3), activation='relu', padding='same') (rc3)\n",
    "rc3 = Conv2D(nerual*4, (3, 3), activation='relu', padding='same') (rc3)\n",
    "#rc3 = Add() ([c3,rc3])\n",
    "p2 = Dropout(0.5) (rc3)\n",
    "\n",
    "c3 = Conv2D(nerual*4, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(nerual*4, (3, 3), activation=None, padding='same') (c3)\n",
    "rc3 = BatchNormalization()(c3)\n",
    "rc3 = Activation('relu')(rc3)\n",
    "rc3 = Conv2D(nerual*4, (3, 3), activation='relu', padding='same') (rc3)\n",
    "rc3 = Conv2D(nerual*4, (3, 3), activation='relu', padding='same') (rc3)\n",
    "#rc3 = Add() ([c3,rc3])\n",
    "p2 = Dropout(0.5) (rc3)\n",
    "\n",
    "c3 = Conv2D(nerual*8, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(nerual*8, (3, 3), activation=None, padding='same') (c3)\n",
    "rc3 = BatchNormalization()(c3)\n",
    "rc3 = Activation('relu')(rc3)\n",
    "rc3 = Conv2D(nerual*4, (3, 3), activation='relu', padding='same') (rc3)\n",
    "rc3 = Conv2D(nerual*4, (3, 3), activation='relu', padding='same') (rc3)\n",
    "\n",
    "flat = Flatten()(rc3)\n",
    "softmax_layer = Dense(units=2,activation=\"softmax\")(flat)\n",
    "model=Model(inputs=input_layer,outputs=softmax_layer)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Two Network models are tried. The fisrt is simple CNN with dropout. Second is a larger Network with ResNet shortcut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 5s 667us/step - loss: 0.6772 - acc: 0.5900 - val_loss: 0.6615 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66145, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 3s 402us/step - loss: 0.6209 - acc: 0.6676 - val_loss: 0.6672 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.66145\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 3s 433us/step - loss: 0.5856 - acc: 0.7001 - val_loss: 0.7453 - val_acc: 0.6575\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.66145\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 3s 435us/step - loss: 0.5575 - acc: 0.7192 - val_loss: 0.5132 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.66145 to 0.51318, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 3s 429us/step - loss: 0.5320 - acc: 0.7415 - val_loss: 0.5015 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51318 to 0.50153, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 3s 435us/step - loss: 0.5136 - acc: 0.7574 - val_loss: 0.5458 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.50153\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 3s 432us/step - loss: 0.4925 - acc: 0.7719 - val_loss: 0.4730 - val_acc: 0.7765\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.50153 to 0.47301, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 3s 410us/step - loss: 0.4819 - acc: 0.7762 - val_loss: 0.5456 - val_acc: 0.7365\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.47301\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 3s 435us/step - loss: 0.4653 - acc: 0.7919 - val_loss: 0.4973 - val_acc: 0.7835\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.47301\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 3s 427us/step - loss: 0.4484 - acc: 0.8020 - val_loss: 0.4302 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.47301 to 0.43015, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 3s 398us/step - loss: 0.4344 - acc: 0.8055 - val_loss: 0.4189 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.43015 to 0.41889, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 3s 409us/step - loss: 0.4145 - acc: 0.8163 - val_loss: 0.4097 - val_acc: 0.8150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.41889 to 0.40969, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 3s 431us/step - loss: 0.3956 - acc: 0.8297 - val_loss: 0.3956 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.40969 to 0.39556, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 3s 422us/step - loss: 0.3850 - acc: 0.8342 - val_loss: 0.3906 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.39556 to 0.39061, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 3s 424us/step - loss: 0.3728 - acc: 0.8474 - val_loss: 0.4271 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.39061\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 3s 433us/step - loss: 0.3546 - acc: 0.8496 - val_loss: 0.3910 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.39061\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 4s 469us/step - loss: 0.3428 - acc: 0.8570 - val_loss: 0.4052 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.39061\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 4s 444us/step - loss: 0.3326 - acc: 0.8661 - val_loss: 0.4256 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.39061\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 3s 426us/step - loss: 0.3165 - acc: 0.8715 - val_loss: 0.3991 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.39061\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 3s 409us/step - loss: 0.2569 - acc: 0.8974 - val_loss: 0.3381 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.39061 to 0.33808, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 3s 406us/step - loss: 0.2379 - acc: 0.9044 - val_loss: 0.3327 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.33808 to 0.33270, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 0.2218 - acc: 0.9127 - val_loss: 0.3405 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.33270\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.2233 - acc: 0.9136 - val_loss: 0.3327 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.33270 to 0.33270, saving model to ./Seconf_net_no_shortcut.model\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 0.2103 - acc: 0.9151 - val_loss: 0.3358 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.33270\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 4s 473us/step - loss: 0.2041 - acc: 0.9210 - val_loss: 0.3541 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.33270\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 0.2027 - acc: 0.9241 - val_loss: 0.3425 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.33270\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 3s 423us/step - loss: 0.1943 - acc: 0.9240 - val_loss: 0.3425 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.33270\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 3s 427us/step - loss: 0.1878 - acc: 0.9290 - val_loss: 0.3426 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.33270\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 4s 454us/step - loss: 0.1890 - acc: 0.9288 - val_loss: 0.3444 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.33270\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 4s 442us/step - loss: 0.1865 - acc: 0.9295 - val_loss: 0.3452 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.33270\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 0.1881 - acc: 0.9284 - val_loss: 0.3462 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.33270\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 3s 435us/step - loss: 0.1861 - acc: 0.9271 - val_loss: 0.3457 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.33270\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 4s 464us/step - loss: 0.1889 - acc: 0.9296 - val_loss: 0.3450 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.33270\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 4s 497us/step - loss: 0.1817 - acc: 0.9303 - val_loss: 0.3454 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.33270\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 4s 440us/step - loss: 0.1870 - acc: 0.9293 - val_loss: 0.3452 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.33270\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 4s 447us/step - loss: 0.1858 - acc: 0.9271 - val_loss: 0.3471 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.33270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f9e2dd160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_reduce = ReduceLROnPlateau(factor=0.1, cooldown=0, patience=5, min_lr=0.e-6)\n",
    "early_stop = EarlyStopping(min_delta=0.001, patience=15)\n",
    "checkpoint = ModelCheckpoint(\"./Second_net.model\", save_best_only=True, verbose=1)\n",
    "model.fit(train_data, train_label,batch_size=32,epochs=100,validation_data=(val_data, val_label),callbacks=[lr_reduce, early_stop,checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data=pd.read_json('test.json')\n",
    "test_data=np.array(testing_data['data'][0:int(testing_data.shape[0]*4/5)].tolist())\n",
    "ffin=[]\n",
    "fin=[]\n",
    "tmp=[];\n",
    "ttmp=[];\n",
    "for i in test_data:\n",
    "    for k in range(32):\n",
    "        for j in range(32):\n",
    "            ttmp.append(i[k*32+j]/255.0)\n",
    "            ttmp.append(i[k*32+j+1024]/255.0)\n",
    "            ttmp.append(i[k*32+j+2048]/255.0)\n",
    "            tmp.append(ttmp)\n",
    "            ttmp=[]\n",
    "        fin.append(tmp)\n",
    "        tmp=[]\n",
    "    ffin.append(fin)\n",
    "    fin=[]\n",
    "\n",
    "test_data=np.array(ffin)\n",
    "ffin=[]\n",
    "#get test_data in 32x32x3 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_label=np.array(testing_data['labels'][0:int(testing_data.shape[0]*4/5)].tolist())\n",
    "test_label=np_utils.to_categorical(test_label, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction=model.predict(test_data)\n",
    "prediction=np.argmax(prediction,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test_label=np.argmax(test_label,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1417\n",
      "211\n",
      "0.8703931203931204\n"
     ]
    }
   ],
   "source": [
    "ra=0\n",
    "wa=0\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i]==test_label[i]:\n",
    "        ra+=1\n",
    "    else:\n",
    "        wa += 1\n",
    "\n",
    "print('Acc:',1.0*ra/(ra+wa))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
